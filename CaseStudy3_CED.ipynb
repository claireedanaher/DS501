{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 3 : Textual analysis of movie reviews\n",
    "\n",
    "** Due Date: November 16, 2017, BEFORE the beginning of class at 6:00pm **\n",
    "\n",
    "NOTE: There are always last minute issues submitting the case studies. DO NOT WAIT UNTIL THE LAST MINUTE!\n",
    "\n",
    "*------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.conversational-technologies.com/nldemos/nlWordle.GIF\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
    "\n",
    "    member 1\n",
    "    \n",
    "    member 2\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desired outcome of the case study.**\n",
    "* In this case study we will look at movie reviews from the v2.0 polarity dataset comes from\n",
    "the http://www.cs.cornell.edu/people/pabo/movie-review-data.\n",
    "    * It contains written reviews of movies divided into positive and negative reviews.\n",
    "* As in Case Study 2 idea is to *analyze* the data set, make *conjectures*, support or refute those conjectures with *data*, and *tell a story* about the data!\n",
    "    \n",
    "**Required Readings:** \n",
    "* This case study will be based upon the scikit-learn Python library\n",
    "* We will build upon the turtorial \"Working With Text Data\" which can be found at http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "**Case study assumptions:**\n",
    "* You have access to a python installation\n",
    "\n",
    "**Required Python libraries:**\n",
    "* Numpy (www.numpy.org) (should already be installed from Case Study 2)\n",
    "* Matplotlib (matplotlib.org) (should already be installed from Case Study 2)\n",
    "* Scikit-learn (scikit-learn.org) (avaiable from Anaconda)\n",
    "* You are also welcome to use the Python Natural Language Processing Toolkit (www.nltk.org) (though it is not required).\n",
    "\n",
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (20 points): Complete Exercise 2: Sentiment Analysis on movie reviews from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assuming that you have downloaded the scikit-learn source code (depending on your distribution, you may need to download this directly from Gitub at https://github.com/scikit-learn/scikit-learn):\n",
    "    * The data cane be downloaded using doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py\n",
    "    * A skeleton for the solution can be found in doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py\n",
    "    * A completed solution can be found in doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py\n",
    "* **It is ok to use the solution provided in the scikit-learn distribution as a starting place for your work.**\n",
    "\n",
    "### Modify the solution to Exercise 2 so that it can run in this iPython notebook\n",
    "* This will likely involved moving around data files and/or small modifications to the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz (3 MB)\n",
      "Decompressing review_polarity.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from contextlib import closing\n",
    "try:\n",
    "    from urllib import urlopen\n",
    "except ImportError:\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "\n",
    "URL = (\"http://www.cs.cornell.edu/people/pabo/\"\n",
    "       \"movie-review-data/review_polarity.tar.gz\")\n",
    "\n",
    "ARCHIVE_NAME = URL.rsplit('/', 1)[1]\n",
    "DATA_FOLDER = \"txt_sentoken\"\n",
    "\n",
    "\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "\n",
    "    if not os.path.exists(ARCHIVE_NAME):\n",
    "        print(\"Downloading dataset from %s (3 MB)\" % URL)\n",
    "        opener = urlopen(URL)\n",
    "        with open(ARCHIVE_NAME, 'wb') as archive:\n",
    "            archive.write(opener.read())\n",
    "\n",
    "    print(\"Decompressing %s\" % ARCHIVE_NAME)\n",
    "    with closing(tarfile.open(ARCHIVE_NAME, \"r:gz\")) as archive:\n",
    "        archive.extractall(path='.')\n",
    "    os.remove(ARCHIVE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n",
      "0 params - {'vect__ngram_range': (1, 1)}; mean - 0.84; std - 0.01\n",
      "1 params - {'vect__ngram_range': (1, 2)}; mean - 0.86; std - 0.01\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.85      0.84      0.85       258\n",
      "        pos       0.83      0.84      0.84       242\n",
      "\n",
      "avg / total       0.84      0.84      0.84       500\n",
      "\n",
      "[[217  41]\n",
      " [ 38 204]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Build a sentiment analysis / polarity model\n",
    "Sentiment analysis can be casted as a binary text classification problem,\n",
    "that is fitting a linear classifier on features extracted from the text\n",
    "of the user messages so as to guess wether the opinion of the author is\n",
    "positive or negative.\n",
    "In this examples we will use a movie review dataset.\n",
    "\"\"\"\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "# License: Simplified BSD\n",
    "\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # NOTE: we put the following in a 'if __name__ == \"__main__\"' protected\n",
    "    # block to be able to use a multi-core grid search that also works under\n",
    "    # Windows, see: http://docs.python.org/library/multiprocessing.html#windows\n",
    "    # The multiprocessing module is used as the backend of joblib.Parallel\n",
    "    # that is used when n_jobs != 1 in GridSearchCV\n",
    "\n",
    "    # the training data folder must be passed as first argument\n",
    "    movie_reviews_data_folder = sys.argv[1]\n",
    "    dataset = load_files('txt_sentoken', shuffle=False)\n",
    "    print(\"n_samples: %d\" % len(dataset.data))\n",
    "\n",
    "    # split the dataset in training and test set:\n",
    "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
    "    \n",
    "   \n",
    "    # TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "    # that are too rare or too frequent\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
    "        ('clf', LinearSVC(C=1000)),\n",
    "    ])\n",
    "\n",
    "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "    # more useful.\n",
    "    # Fit the pipeline on the training set using grid search for the parameters\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    }\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "    grid_search.fit(docs_train, y_train)\n",
    "\n",
    "    # TASK: print the mean and std for each candidate along with the parameter\n",
    "    # settings for all the candidates explored by grid search.\n",
    "    n_candidates = len(grid_search.cv_results_['params'])\n",
    "    for i in range(n_candidates):\n",
    "        print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "                 % (grid_search.cv_results_['params'][i],\n",
    "                    grid_search.cv_results_['mean_test_score'][i],\n",
    "                    grid_search.cv_results_['std_test_score'][i]))\n",
    "\n",
    "    # TASK: Predict the outcome on the testing set and store it in a variable\n",
    "    # named y_predicted\n",
    "    y_predicted = grid_search.predict(docs_test)\n",
    "\n",
    "    # Print the classification report\n",
    "    print(metrics.classification_report(y_test, y_predicted,\n",
    "                                        target_names=dataset.target_names))\n",
    "\n",
    "    # Print and plot the confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "    print(cm)\n",
    "\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.matshow(cm)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b\"while i am not fond of any writer's use of cheap , easy puns , i am not completely above using them myself when the situation merits it ( witness my review of _pecker_ from a couple of issues ago ) . \\nso here goes : the juvenile , college-set black comedy _dead_man_on_campus_ is dead on arrival . \\nstrait-laced med student josh ( tom everett scott , who manages to remain somewhat likable throughout ) 's blemish-free academic record breaks out into fs , thanks to the influence of his ever-partying roommate , cooper ( mark-paul gosselaar ) , who introduces josh to the sex- and booze-filled nights that come with university life . \\nwith the threat of losing an academic scholarship ( josh ) and a life cleaning toilets for his dad looming ( cooper ) , what are two good-hearted slackers to do ? \\neasy--look for a loophole , which they find in the form of an unbelievable rule in the school charter that states that if a student's ( or students' ) roommate commits suicide , the surviving student ( s ) shall receive straight as . \\nso instead of studying , josh and cooper attempt to seek out the most depressed student out there , move him into their dorm room , and drive him to suicide before the semester ends . \\ndirector alan cohn and screenwriters michael traeger and mike white ( working from a story by anthony abrams and adam larson broder ) take their sweet time to build the head of steam that comes with josh and cooper's diabolical plot . \\nuntil then , the usual boring cliches of college life ( booze , sex , more booze ) fill the time , which is made to feel longer by _saved_by_the_bell_ alumnus gosselaar's sitcom-bred mugging . \\nthat said , once cohn and company do build some comic momentum , they mishandle it . \\nthe introduction of the manic , psychotic cliff ( lochlyn munro ) , a potential roommate for josh and cooper , brings some demented life to the uninspired proceedings before being hastily written out in favor of two less interesting candidates : paranoid nerd buckley ( randy pearlstein ) and british death rocker matt ( corey page ) . \\none wishes that cliff would reappear , but , as they say , be careful what you wish for . \\nnot surprisingly , he does resurface , and it then becomes clear that this is a character that is best taken in a small dose ; almost immediately , his extended boorish and sociopathic antics loses its novelty . \\nthe same can be said about all of _dead_man_on_campus_ . \\nwhatever morbid appeal the far-fetched premise has quickly evaporates , and the self-absorbed characters , especially cooper , pretty much grate from the get-go . \\n_dead_man_ doesn't grow tiresome ; it already _is_ once the clever opening titles are through . \\nas it slogs along to a cheesy , happy-for-all-parties conclusion , _dead_man_ lives up to its title and then some--not only does the movie grow even more tired and die , it still insists on going on . . . \\nlike a zombie . \\n\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_train[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (20 points): Explore the scikit-learn TfidVectorizer class\n",
    "\n",
    "**Read the documentation for the TfidVectorizer class at http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html.** \n",
    "* Define the term frequencyâ€“inverse document frequency (TF-IDF) statistic (http://en.wikipedia.org/wiki/Tf%E2%80%93idf will likely help).\n",
    "* Run the TfidVectorizer class on the training data above (docs_train).\n",
    "* Explore the min_df and max_df parameters of TfidVectorizer.  What do they mean? How do they change the features you get?\n",
    "* Explore the ngram_range parameter of TfidVectorizer.  What does it mean? How does it change the features you get? (Note, large values  of ngram_range may take a long time to run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vampire                                            Score: 13.418208164942516\n",
      "hong                                               Score: 12.982206501425248\n",
      "legend                                             Score: 12.851302930516814\n",
      "chan                                               Score: 12.676152813506459\n",
      "star trek                                          Score: 11.283368720723153\n",
      "truman                                             Score: 11.143198986742938\n",
      "carrey                                             Score: 10.88036718816322\n",
      "tarantino                                          Score: 10.096145594727641\n",
      "tarzan                                             Score: 9.892845810777738\n",
      "spawn                                              Score: 8.19768817282266\n",
      "vampire                                            Score: 13.418208164942516\n",
      "hong                                               Score: 12.982206501425248\n",
      "legend                                             Score: 12.851302930516814\n",
      "chan                                               Score: 12.676152813506459\n",
      "star trek                                          Score: 11.283368720723153\n",
      "truman                                             Score: 11.143198986742938\n",
      "carrey                                             Score: 10.88036718816322\n",
      "tarantino                                          Score: 10.096145594727641\n",
      "tarzan                                             Score: 9.892845810777738\n",
      "spawn                                              Score: 8.19768817282266\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def vector(doc,val_max_feature,val_min_df,val_max_df,val_ngram_range):\n",
    "    vectorizor=TfidfVectorizer(doc,max_features=val_max_feature,\n",
    "                           min_df=val_min_df,max_df=val_max_df,\n",
    "                           stop_words='english',\n",
    "                           ngram_range=val_ngram_range)\n",
    "\n",
    "    tfid_result=vectorizor.fit_transform(docs_train)\n",
    "\n",
    "\n",
    "\n",
    "def display_scores(vectorizer, tfidf_result):\n",
    "    # http://stackoverflow.com/questions/16078015/\n",
    "    scores = zip(vectorizer.get_feature_names(),\n",
    "                 np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    end=len(sorted_scores)\n",
    "    for item in sorted_scores[end-10:end]:\n",
    "        print (\"{0:50} Score: {1}\".format(item[0], item[1]))\n",
    "\n",
    "        \n",
    "doc=docs_train\n",
    "val_max_feature=200\n",
    "val_min_df=0.0\n",
    "val_max_df=0.1\n",
    "val_ngram_range=(1,5)\n",
    "\n",
    "vector(doc,val_max_feature,val_min_df,val_max_df,val_ngram_range)\n",
    "\n",
    "display_scores(vectorizor,tfid_result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val_max_feature=200\n",
    "val_min_df=0.2\n",
    "val_max_df=0.9\n",
    "val_ngram_range=(1,5)\n",
    "\n",
    "vector(doc,val_max_feature,val_min_df,val_max_df,val_ngram_range)\n",
    "\n",
    "display_scores(vectorizor,tfid_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*------------------------\n",
    "\n",
    "## Problem 3 (20 points): Machine learning algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based upon Problem 2 pick some parameters for TfidfVectorizer\n",
    "    * \"fit\" your TfidfVectorizer using docs_train\n",
    "    * Compute \"Xtrain\", a Tf-idf-weighted document-term matrix using the transform function on docs_train\n",
    "    * Compute \"Xtest\", a Tf-idf-weighted document-term matrix using the transform function on docs_test\n",
    "    * Note, be sure to use the same Tf-idf-weighted class (**\"fit\" using docs_train**) to transform **both** docs_test and docs_train\n",
    "* Examine two classifiers provided by scikit-learn \n",
    "    * LinearSVC\n",
    "    * KNeighborsClassifier\n",
    "    * Try a number of different parameter settings for each and judge your performance using a confusion matrix (see Problem 1 for an example).\n",
    "* Does one classifier, or one set of parameters work better?\n",
    "    * Why do you think it might be working better?\n",
    "* For a particular choice of parameters and classifier, look at 2 examples where the prediction was incorrect.\n",
    "    * Can you conjecture on why the classifier made a mistake for this prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*------------------------\n",
    "\n",
    "## Problem 4 (20 points): Open Ended Question:  Finding the right plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can you find a two dimensional plot in which the positive and negative reviews are separated?\n",
    "    * This problem is hard since you will likely have thousands of features for review, and you will need to transform these thousands of features into just two numbers (so that you can make a 2D plot).\n",
    "* Note, I was not able to find such a plot myself!\n",
    "    * So, this problem is about **trying** but perhaps **not necessarily succeeding**!\n",
    "* I tried two things, neither of which worked very well.\n",
    "    * I first plotted the length of the review versus the number of features we compute that are in that review\n",
    "    * Second I used Principle Component Analysis on a subset of the features.\n",
    "* Can you do better than I did!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report: communicate the results (20 points)\n",
    "\n",
    "(1) (5 points) What data you collected?\n",
    "\n",
    "(2) (5 points) Why this topic is interesting or important to you? (Motivations)\n",
    "\n",
    "(3) (5 points) How did you analyse the data?\n",
    "\n",
    "(4) (5 points) What did you find in the data?\n",
    "(please include figures or tables in the report, but no source code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slides (for 10 minutes of presentation) (20 points)\n",
    "\n",
    "\n",
    "1. (5 points) Motivation about the data collection, why the topic is interesting to you. \n",
    "\n",
    "2. (10 points) Communicating Results (figure/table)\n",
    "\n",
    "3. (5 points) Story telling (How all the parts (data, analysis, result) fit together as a story?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study. \n",
    "\n",
    "* ** Report**: please prepare a report (less than 10 pages) to report what you found in the data.\n",
    "    * What is the relationship between this topic and Business Intelligence?\n",
    "    * How did you analyse the data?\n",
    "    * What did you find in the data? \n",
    "    * What conjectures did you make and how did you support or disprove them using data?\n",
    "    * Did you find anything suprising in the data?\n",
    "    * What business decision do you think this data could help answer?  Why?\n",
    "\n",
    "   (please include figures or tables in the report, **but no source code**)\n",
    "\n",
    "*Please compress all the files into a single zipped file.*\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Please submit through email to Prof. Paffenroth (rcpaffenroth@wpi.edu) *and* the TA Yingnan Liu (yliu18@wpi.edu).\n",
    "\n",
    "#### We auto-process the submissions so make sure your subject line is *exactly*:\n",
    "\n",
    "### DS501 Case Study 3 Team ??\n",
    "\n",
    "#### where ?? is your team number.\n",
    "        \n",
    "** Note: Each team just needs to submits one submission **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nteract": {
   "version": "0.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
